# graduate-courses-notes
Contains information, notes, and personal opinion on a couple of Graduate Level courses I took while studying on my own to improve my Machine Learning. and Software Engineering Skills.
I arranged the courses by the offering university.
## Stanford University
I took a couple of courses from Stanford, especially right at the beginning of my training. The include:
1. [Statistical Learning](https://lagunita.stanford.edu/courses/HumanitiesScience/StatLearning/Winter2014/courseware/d9820868fd3642f19ee45e273e3dfafa/).
This course is fantastic, and mainly revolves around the book [Elements of Statistical Learning](https://web.stanford.edu/~hastie/Papers/ESLII.pdf). It's a great foundation in Machine Learning, and covers the major ML models, like Linear Regression, Logistic Regression, Decision Trees, Gradient Boosting (one of my favourite!), and many others. The Course Authors are among the inventors of a couple of these algorithm and are extremely well known in their fields. In fact, understanding this course automatically leads one to understand the theory behind most ensembling algorithms, like Random Forests and Gradient Boosted Decision Trees.
	- Programming Language Used: `R`
	- Course Difficulty: Medium
	- Mathematics Content: Average

2. [Deep Learning for Natural Language Processing](http://cs224d.stanford.edu/)
This is one of my favorite course on Natural Language Processing (NLP). It was one of the first courses in NLP I took, when preparing for the [Allen AI Science Challenge](https://www.kaggle.com/c/the-allen-ai-science-challenge), where I scored Top 5% in the world. In fact, contrary to most folks who got into Deep Learning via Image Processing & Computer Vision, I got into Deep Learning via text processing. It was a bit hard, especially the Maths was very advanced and occasionally multi-variable calculus proofs were required, however through reading of other NLP papers and materials I managed to finish auditing the course to prepare my solution to the competition. The course covers almost everything in DL for Natural Language processing, from word vectors to advanced Recurrent Neural Networks for sequence prediction, like Long Short Term Memory Networks (LSTMs).

3. []()
